# OS 답변 77-80

## 77. L1, L2 캐시에 대해 설명해 주세요.

**기본 개념**

L1과 L2 캐시는 CPU 코어에 가장 가까운 캐시 계층으로, 프로세서 성능에 가장 큰 영향을 미칩니다. 각각 다른 특성과 역할을 가지고 있습니다.

**L1 캐시의 특징**

L1 캐시는 CPU 코어 내부에 위치한 가장 빠른 캐시입니다. 명령어 캐시와 데이터 캐시로 분리되어 동시 접근이 가능한 하버드 아키텍처를 사용합니다. 접근 속도는 1-2 클럭 사이클로 매우 빠르지만, 용량은 32KB에서 64KB로 작습니다. 각 코어마다 전용 L1 캐시를 가지므로 코어 간 공유가 없습니다. 히트율이 매우 높아야 전체 성능이 좋아집니다.

**L1 명령어 캐시와 데이터 캐시**

L1 명령어 캐시는 실행할 기계어 명령어를 저장합니다. 프로그램 카운터 주변의 명령어를 미리 가져와서 파이프라인에 공급합니다. 분기 예측과 함께 동작하여 명령어 공급을 원활하게 합니다. L1 데이터 캐시는 로드와 스토어 명령어가 접근하는 데이터를 저장합니다. 두 캐시가 분리되어 명령어 페치와 데이터 접근이 동시에 가능하여 성능이 향상됩니다.

**L2 캐시의 특징**

L2 캐시는 L1보다 느리지만 용량이 큰 중간 계층입니다. 일반적으로 256KB에서 512KB 정도이며, 접근 시간은 10 사이클 정도입니다. 최신 아키텍처에서는 각 코어에 전용 L2 캐시를 제공하지만, 일부는 여러 코어가 공유하기도 합니다. L1 미스 시 L2에서 데이터를 찾아 L1으로 가져옵니다. 통합 캐시로 명령어와 데이터를 모두 저장합니다.

**계층 간 상호작용**

CPU가 데이터를 요청하면 먼저 L1 캐시를 확인합니다. L1 히트면 즉시 데이터를 반환하고, 미스면 L2를 확인합니다. L2 히트면 데이터를 L1과 CPU에 동시에 전달하고, L2 미스면 L3나 메인 메모리를 확인합니다. 포괄적 계층에서는 L2가 L1의 모든 데이터를 포함하여 일관성 유지가 쉽습니다.

**캐시 라인과 어소시에이티비티**

캐시는 캐시 라인 단위로 데이터를 저장하며, 일반적으로 64바이트입니다. L1은 8-way 어소시에이티브, L2는 16-way 같은 세트 어소시에이티브 구조를 사용합니다. 어소시에이티비티가 높을수록 캐시 충돌이 줄지만 하드웨어 복잡도가 증가합니다. L1은 속도를 위해 낮은 어소시에이티비티를, L2는 히트율을 위해 높은 어소시에이티비티를 선택합니다.

**성능 최적화**

프로그래머는 L1 캐시 크기를 고려하여 핫 루프의 작업 세트를 최소화해야 합니다. 루프 내에서 접근하는 데이터가 32KB 이내면 L1에 모두 들어가 매우 빠릅니다. 루프 블로킹 기법으로 큰 데이터를 작은 블록으로 나누어 처리합니다. 구조체 패딩을 조정하여 캐시 라인을 효율적으로 사용합니다.

**측정과 분석**

perf stat 명령으로 L1과 L2 캐시 미스율을 측정할 수 있습니다. perf record와 perf report로 어느 코드에서 캐시 미스가 발생하는지 분석합니다. Intel VTune이나 AMD uProf 같은 프로파일러는 더 상세한 캐시 분석을 제공합니다. 캐시 미스가 성능 병목이면 알고리즘이나 자료구조를 개선해야 합니다.

**실무 활용**

고성능 컴퓨팅에서는 캐시 최적화가 핵심입니다. 행렬 곱셈 같은 연산은 타일링으로 L1 캐시를 최대한 활용합니다. 데이터베이스 인덱스는 캐시에 잘 맞도록 B-tree 노드 크기를 조정합니다. 게임 엔진은 구조체 배열 방식으로 캐시 친화적인 데이터 레이아웃을 사용합니다. 캐시 이해는 성능 최적화의 기본입니다.

---

## 78. 캐시에 올라오는 데이터는 어떻게 관리되나요?

**기본 개념**

캐시는 제한된 용량으로 메모리의 일부만 저장하므로, 어떤 데이터를 캐시에 유지할지 결정하는 관리 정책이 필요합니다. 교체 정책, 쓰기 정책, 일관성 정책이 핵심입니다.

**캐시 교체 정책**

캐시가 가득 찬 상태에서 새 데이터를 가져와야 할 때, 기존 데이터 중 하나를 제거해야 합니다. LRU는 가장 오래 사용되지 않은 데이터를 제거하는 정책으로, 시간적 지역성을 잘 활용합니다. LFU는 가장 적게 사용된 데이터를 제거하지만 구현이 복잡합니다. 실제 하드웨어는 근사 LRU나 FIFO, Random 같은 단순한 정책을 사용하여 하드웨어 복잡도를 줄입니다.

**쓰기 정책 - Write-Through**

Write-Through는 캐시에 쓸 때 동시에 메모리에도 씁니다. 캐시와 메모리가 항상 일치하여 일관성 유지가 쉽고, 캐시 실패나 전원 손실 시에도 데이터가 안전합니다. 하지만 모든 쓰기가 메모리까지 전달되어 느리고, 메모리 대역폭을 많이 소비합니다. 쓰기 버퍼를 사용하여 CPU는 블로킹되지 않지만 여전히 메모리 트래픽이 많습니다.

**쓰기 정책 - Write-Back**

Write-Back은 캐시에만 쓰고, 해당 캐시 라인이 제거될 때 메모리에 씁니다. 캐시에 Dirty 비트를 두어 수정 여부를 추적합니다. 여러 번 쓰기가 발생해도 한 번만 메모리에 쓰므로 훨씬 빠르고 효율적입니다. 하지만 캐시와 메모리가 일시적으로 불일치하며, 복잡한 일관성 프로토콜이 필요합니다. 현대 CPU는 대부분 Write-Back을 사용합니다.

**캐시 할당 정책**

읽기 미스 시 데이터를 캐시에 할당하는 것은 당연하지만, 쓰기 미스 시 정책이 나뉩니다. Write-Allocate는 쓰기 미스 시에도 캐시에 할당하여 후속 접근을 빠르게 합니다. No-Write-Allocate는 쓰기 미스 시 캐시를 거치지 않고 바로 메모리에 씁니다. Write-Back은 보통 Write-Allocate와 함께 사용하고, Write-Through는 No-Write-Allocate와 함께 사용합니다.

**프리페칭**

캐시는 요청받은 데이터뿐 아니라 인접한 데이터도 미리 가져옵니다. 하드웨어 프리페처는 접근 패턴을 감지하여 자동으로 다음 데이터를 예측합니다. 순차 접근이나 스트라이드 패턴을 인식하여 미리 캐시에 적재합니다. 소프트웨어 프리페치 명령어로 프로그래머가 명시적으로 힌트를 줄 수도 있습니다. 효과적인 프리페칭은 캐시 미스를 크게 줄입니다.

**멀티코어 캐시 일관성**

여러 코어가 같은 데이터를 캐시에 가지면, 한 코어의 쓰기가 다른 코어에 보여야 합니다. MESI나 MOESI 같은 캐시 일관성 프로토콜이 이를 관리합니다. 캐시 라인의 상태를 Modified, Exclusive, Shared, Invalid로 구분하여 추적합니다. 한 코어가 수정하면 다른 코어의 복사본을 무효화하거나 업데이트합니다. 이는 하드웨어가 자동으로 처리하여 프로그래머는 투명하게 사용합니다.

**캐시 라인 크기의 영향**

캐시 라인이 크면 공간 지역성을 잘 활용하지만, False Sharing 문제가 발생할 수 있습니다. 두 스레드가 서로 다른 변수를 접근하더라도, 같은 캐시 라인에 있으면 일관성 프로토콜로 인해 성능이 저하됩니다. 멀티스레드 프로그래밍에서는 변수를 캐시 라인 경계에 맞춰 배치하여 False Sharing을 방지합니다.

**실무 최적화**

캐시 정책을 이해하고 이에 맞는 코드를 작성합니다. 순차 접근으로 프리페칭을 활용하고, 데이터 재사용으로 교체를 줄입니다. Write-Back 특성상 읽기보다 쓰기가 더 비용이 크므로, 불필요한 쓰기를 제거합니다. False Sharing을 피하기 위해 스레드별 데이터를 분리합니다. 프로파일러로 캐시 동작을 측정하고 개선합니다.

---

## 79. 캐시간의 동기화는 어떻게 이루어지나요?

**기본 개념**

멀티코어 시스템에서 각 코어는 독립적인 캐시를 가지므로, 같은 메모리 주소를 여러 캐시가 복사할 수 있습니다. 한 코어가 데이터를 수정하면 다른 코어의 캐시도 업데이트되어야 일관성이 유지됩니다. 이를 캐시 일관성 문제라고 합니다.

**MESI 프로토콜**

MESI는 가장 널리 사용되는 캐시 일관성 프로토콜로, Modified, Exclusive, Shared, Invalid 네 가지 상태를 사용합니다. Modified 상태는 캐시 라인이 수정되었고 메모리와 다르며, 이 코어만 복사본을 가집니다. Exclusive 상태는 수정되지 않았지만 이 코어만 가지고 있습니다. Shared 상태는 여러 코어가 복사본을 가지며 메모리와 일치합니다. Invalid 상태는 캐시 라인이 유효하지 않습니다.

**MESI 상태 전이**

읽기 요청 시 다른 코어가 가지고 있으면 Shared로, 아니면 Exclusive로 진입합니다. 쓰기 요청 시 다른 코어의 복사본을 무효화하고 Modified로 전이합니다. Shared 상태에서 쓰기가 발생하면 다른 코어에 무효화 메시지를 보내고 Modified가 됩니다. Modified 캐시 라인이 제거되면 메모리에 쓰고, 다른 코어가 요청하면 데이터를 제공합니다.

**스누핑 프로토콜**

스누핑은 모든 캐시가 버스를 감시하여 다른 캐시의 동작을 관찰합니다. 한 코어가 메모리 요청을 버스에 보내면, 다른 코어들이 자신의 캐시를 확인합니다. 해당 데이터를 가진 캐시가 응답하거나 무효화 처리를 수행합니다. 버스 기반 시스템에서 효과적이지만, 코어 수가 많으면 버스 대역폭이 병목이 됩니다.

**디렉토리 기반 프로토콜**

대규모 시스템에서는 디렉토리가 각 캐시 라인의 위치를 추적합니다. 캐시 미스 시 디렉토리에 질의하여 어느 코어가 데이터를 가지는지 확인합니다. 필요한 코어들에게만 메시지를 보내므로 확장성이 좋습니다. NUMA 시스템이나 많은 코어를 가진 서버 프로세서가 사용합니다. 복잡도가 높지만 대역폭 낭비가 적습니다.

**캐시 일관성 트래픽**

캐시 동기화는 상당한 버스 트래픽을 발생시킵니다. 한 코어의 쓰기가 다른 코어들에 무효화 메시지를 유발합니다. False Sharing으로 인해 실제 공유되지 않는 데이터도 일관성 트래픽을 발생시킵니다. 이는 멀티코어 성능의 주요 병목이며, 프로그래머가 최소화해야 합니다.

**메모리 배리어**

캐시 일관성은 하드웨어가 보장하지만, 메모리 연산의 순서는 보장하지 않을 수 있습니다. 메모리 배리어나 펜스 명령어로 특정 순서를 강제합니다. C++의 atomic이나 volatile, Java의 volatile이 메모리 배리어를 포함합니다. 이는 컴파일러와 CPU의 재배치를 방지하여 올바른 동기화를 보장합니다.

**실무 고려사항**

멀티스레드 프로그램에서는 False Sharing을 피하기 위해 독립적인 변수를 캐시 라인 경계에 맞춥니다. 읽기 전용 데이터는 여러 코어가 안전하게 공유할 수 있어 성능이 좋습니다. 쓰기가 빈번한 변수는 가능한 한 하나의 코어에서만 접근하도록 설계합니다. 불필요한 동기화를 제거하여 캐시 일관성 오버헤드를 줄입니다. 프로파일러로 캐시 일관성 미스를 측정하고 최적화합니다.

---

## 80. 캐시 메모리의 Mapping 방식에 대해 설명해 주세요.

**기본 개념**

캐시 매핑 방식은 메모리 주소를 캐시의 어느 위치에 저장할지 결정하는 방법입니다. 직접 매핑, 완전 연관 매핑, 세트 연관 매핑의 세 가지 기본 방식이 있으며, 각각 하드웨어 복잡도와 성능의 trade-off가 있습니다.

**직접 매핑**

직접 매핑은 메모리 주소를 캐시 라인 수로 나눈 나머지로 위치를 결정합니다. 각 메모리 블록이 캐시의 정확히 한 위치에만 매핑됩니다. 하드웨어 구현이 가장 간단하고 빠르며, 인덱스 계산만으로 위치를 찾습니다. 하지만 캐시 충돌이 자주 발생합니다. 주소가 다르더라도 같은 캐시 라인에 매핑되면, 서로 교체되며 히트율이 낮아집니다. 작은 임베디드 시스템에서 사용됩니다.

**완전 연관 매핑**

완전 연관 매핑은 메모리 블록을 캐시의 아무 위치에나 저장할 수 있습니다. 모든 캐시 라인을 검색하여 태그를 비교해야 하므로 하드웨어가 복잡하고 느립니다. 하지만 캐시 충돌이 최소화되어 히트율이 가장 높습니다. 교체 정책을 자유롭게 선택할 수 있어 LRU 같은 정교한 알고리즘을 사용할 수 있습니다. 용량이 작은 TLB나 일부 L1 캐시에서 사용됩니다.

**세트 연관 매핑**

세트 연관 매핑은 직접 매핑과 완전 연관 매핑의 절충안입니다. 캐시를 여러 세트로 나누고, 각 메모리 블록은 특정 세트에 매핑됩니다. 세트 내에서는 아무 위치에나 저장할 수 있습니다. N-way 세트 연관은 각 세트에 N개의 라인을 가집니다. 예를 들어, 8-way는 8개 라인 중 하나를 선택할 수 있습니다. 직접 매핑보다 충돌이 적고, 완전 연관보다 하드웨어가 간단합니다. 현대 CPU의 대부분은 세트 연관 방식을 사용합니다.

**주소 분할**

캐시는 메모리 주소를 태그, 인덱스, 오프셋 세 부분으로 나눕니다. 오프셋은 캐시 라인 내의 바이트 위치를 지정하며, 보통 6비트로 64바이트를 표현합니다. 인덱스는 어느 세트를 사용할지 결정합니다. 태그는 세트 내에서 실제 메모리 블록을 식별합니다. 캐시 접근 시 인덱스로 세트를 찾고, 태그를 비교하여 히트 여부를 판단합니다.

**연관도의 영향**

연관도가 높을수록 히트율이 향상되지만, 하드웨어가 복잡하고 전력 소비가 증가합니다. 모든 way를 병렬로 검색해야 하므로 비교기 수가 증가합니다. 일반적으로 8-way에서 16-way가 성능과 비용의 적절한 균형점입니다. L1 캐시는 속도를 위해 낮은 연관도를, L2/L3는 히트율을 위해 높은 연관도를 사용합니다.

**교체 정책과의 관계**

직접 매핑은 선택의 여지가 없어 교체 정책이 필요 없습니다. 세트 연관은 세트 내에서 교체할 라인을 선택해야 합니다. LRU가 이상적이지만 하드웨어 비용이 높아, 근사 LRU나 Random을 사용합니다. 완전 연관은 모든 라인 중에서 선택하므로 정교한 LRU 구현이 가능하지만 복잡합니다.

**실무 예시**

Intel Core i7의 L1 데이터 캐시는 8-way 세트 연관, 32KB입니다. 64바이트 캐시 라인, 64세트로 구성됩니다. L2는 4-way 또는 8-way, 256KB입니다. L3는 16-way, 8MB 이상입니다. ARM Cortex-A는 4-way L1, 8-way L2를 많이 사용합니다. 프로그래머는 연관도를 직접 제어할 수 없지만, 접근 패턴을 최적화하여 충돌을 줄일 수 있습니다.

**성능 최적화**

스트라이드 접근이 캐시 라인 수의 배수이면 충돌이 발생하기 쉽습니다. 배열 크기나 접근 패턴을 조정하여 이를 피합니다. 구조체 배열보다 배열의 구조체가 공간 지역성을 높여 캐시 활용률을 개선합니다. 루프 타일링으로 작업 세트를 캐시 크기에 맞춥니다. 프로파일러로 캐시 충돌을 분석하고 데이터 레이아웃을 최적화합니다.
