# OS 답변 73-76

## 73. Thread Pool을 사용한다고 가정하면, 어떤 기준으로 스레드의 수를 결정할 것인가요?

**기본 개념**

Thread Pool의 크기는 애플리케이션 성능에 큰 영향을 미칩니다. 너무 적으면 CPU가 유휴 상태가 되고, 너무 많으면 컨텍스트 스위칭 오버헤드와 메모리 낭비가 발생합니다. 작업 특성, 하드웨어 자원, 시스템 요구사항을 고려하여 결정해야 합니다.

**CPU 집약적 작업**

CPU 연산이 주를 이루는 작업은 CPU 코어 수에 맞추는 것이 이상적입니다. 코어 수보다 많은 스레드를 생성해도 병렬성이 증가하지 않고, 컨텍스트 스위칭만 늘어납니다. 일반적으로 코어 수 플러스 1 정도가 적합합니다. 추가 스레드는 페이지 폴트나 캐시 미스 대기 시간을 활용합니다. 과학 계산, 이미지 처리, 암호화 같은 작업이 해당됩니다.

**I/O 집약적 작업**

I/O 대기 시간이 긴 작업은 더 많은 스레드가 필요합니다. 한 스레드가 I/O를 대기하는 동안 다른 스레드가 CPU를 사용할 수 있습니다. 공식으로는 코어 수 곱하기 2에서 시작하여 조정합니다. 네트워크 요청, 데이터베이스 쿼리, 파일 읽기/쓰기가 많은 웹 서버나 API 서버가 해당됩니다. I/O 대기 비율이 높을수록 더 많은 스레드가 효율적입니다.

**혼합 작업**

CPU와 I/O가 혼합된 작업은 경험적 공식을 사용합니다. 스레드 수는 코어 수 곱하기 1 더하기 대기 시간 나누기 계산 시간입니다. 대기 시간이 계산 시간보다 길면 더 많은 스레드가 필요하고, 반대면 적은 스레드가 적합합니다. 프로파일링을 통해 실제 비율을 측정하고 최적값을 찾아야 합니다.

**시스템 자원 고려**

메모리 제약도 중요한 요소입니다. 각 스레드는 스택 메모리를 소비하므로, 스레드가 많으면 메모리 부족이 발생할 수 있습니다. 리눅스 기본 스택 크기는 8MB이므로, 1000개 스레드면 8GB 메모리가 필요합니다. 파일 디스크립터, 네트워크 연결 같은 시스템 자원 한계도 확인해야 합니다. ulimit 설정을 조정하여 제한을 늘릴 수 있습니다.

**동적 조정**

고정 크기 대신 동적으로 조정하는 전략도 있습니다. 작업 큐 크기와 처리 속도를 모니터링하여 스레드를 추가하거나 제거합니다. Java의 ThreadPoolExecutor는 코어 풀 크기, 최대 풀 크기, 유휴 시간을 설정할 수 있습니다. 부하가 증가하면 최대 크기까지 늘리고, 감소하면 코어 크기로 축소합니다. 탄력적 확장으로 자원을 효율적으로 사용합니다.

**실무 적용**

초기값은 이론적 공식으로 설정하고, 부하 테스트를 통해 최적화합니다. 실제 환경에서 모니터링하며 처리량, 응답 시간, CPU 사용률을 측정합니다. 스레드 덤프와 프로파일러로 병목 지점을 파악합니다. A/B 테스트로 다양한 설정을 비교합니다. 클라우드 환경에서는 오토스케일링과 연계하여 인스턴스와 스레드를 함께 조정합니다.

**안티패턴 회피**

무조건 많은 스레드를 생성하는 것은 오히려 성능을 저하시킵니다. 컨텍스트 스위칭 비용과 스케줄러 부하가 증가합니다. 캐시 미스율이 높아져 메모리 대역폭이 병목이 됩니다. 적절한 크기를 찾는 것이 무조건 많이 만드는 것보다 중요합니다. 측정과 분석을 기반으로 결정해야 합니다.

---

## 74. 어떤 데이터를 정렬 하려고 합니다. 어떤 방식의 전략을 사용하는 것이 가장 안전하면서도 좋은 성능을 낼 수 있을까요?

**기본 개념**

멀티스레드 환경에서 데이터를 안전하게 정렬하려면 동기화 전략과 알고리즘 선택이 모두 중요합니다. Thread Safe하면서도 성능을 최대화하는 접근 방법을 고려해야 합니다.

**불변 데이터 복사 전략**

가장 안전한 방법은 원본 데이터를 복사하여 정렬하는 것입니다. 복사본을 생성하면 다른 스레드가 원본을 읽거나 쓰는 것에 영향받지 않습니다. 정렬이 완료된 후 원자적으로 포인터를 교체하여 결과를 공개합니다. Copy-on-Write 패턴으로 읽기 성능을 유지하면서 안전하게 업데이트합니다. 메모리 오버헤드가 있지만 동시성 문제를 완전히 회피합니다.

**락 기반 보호**

정렬 중인 데이터를 뮤텍스로 보호하는 방법입니다. 읽기-쓰기 락을 사용하면 정렬 중에는 배타적 락을 획득하고, 정렬 후 읽기는 공유 락으로 허용합니다. 정렬 작업이 길면 다른 스레드가 오래 대기할 수 있습니다. 가능하면 정렬 시간을 최소화하거나, 복사 후 정렬하는 방식과 결합합니다.

**병렬 정렬 알고리즘**

대용량 데이터는 병렬 정렬 알고리즘을 사용하여 성능을 향상시킵니다. 병렬 퀵소트는 데이터를 분할하여 각 파티션을 독립적으로 정렬합니다. 병렬 머지소트는 분할 정복 방식으로 자연스럽게 병렬화됩니다. Fork-Join 프레임워크를 활용하여 재귀적 병렬 처리를 구현합니다. C++의 parallel sort나 Java의 parallelSort가 제공됩니다.

**안전한 알고리즘 선택**

머지소트는 안정 정렬이며 최악의 경우에도 O(n log n)을 보장하여 예측 가능합니다. 퀵소트는 평균적으로 빠르지만 최악의 경우 O(n^2)이므로, 인트로소트처럼 하이브리드 알고리즘을 사용합니다. Tim소트는 실제 데이터의 패턴을 활용하여 효율적이며, Python과 Java의 기본 정렬입니다. 안정성이 필요한지, 메모리 제약이 있는지에 따라 선택합니다.

**비블로킹 접근**

락 프리 자료구조를 사용하여 정렬 중에도 조회를 허용할 수 있습니다. ConcurrentSkipListSet 같은 자료구조는 삽입과 조회가 동시에 가능하며, 항상 정렬된 상태를 유지합니다. 추가 비용이 있지만 락 경쟁이 없어 높은 동시성을 제공합니다. 빈번한 업데이트와 조회가 필요한 경우 적합합니다.

**스냅샷 격리**

데이터베이스의 MVCC처럼 버전 관리를 사용할 수 있습니다. 각 업데이트마다 새 버전을 생성하고, 읽기는 특정 시점의 스냅샷을 봅니다. 정렬된 새 버전을 생성하여 원자적으로 현재 버전으로 승격시킵니다. 여러 버전이 공존하여 메모리는 더 사용하지만, 읽기 차단 없이 안전합니다.

**실무 권장사항**

일반적으로는 데이터를 복사하여 정렬한 후 결과를 원자적으로 교체하는 방식이 가장 안전합니다. 메모리가 충분하고 정렬 빈도가 낮으면 이상적입니다. 대용량 데이터는 병렬 정렬로 성능을 확보합니다. 실시간 업데이트가 필요하면 정렬된 자료구조를 처음부터 사용합니다. 상황에 맞는 전략을 선택하고, 성능 테스트로 검증해야 합니다.

---

## 75. 캐시 메모리 및 메모리 계층성에 대해 설명해 주세요.

**기본 개념**

메모리 계층성은 속도와 용량이 다른 여러 계층의 메모리를 조합하여 성능과 비용을 최적화하는 컴퓨터 구조 원리입니다. CPU에서 가까울수록 빠르지만 작고 비싸며, 멀수록 느리지만 크고 저렴합니다.

**메모리 계층 구조**

가장 빠른 것은 CPU 내부의 레지스터로, 수 바이트에서 수십 바이트 용량을 가지며 클럭 사이클 내에 접근 가능합니다. L1 캐시는 수십 KB로 1-2 사이클에 접근합니다. L2 캐시는 수백 KB에서 수 MB로 10 사이클 정도 소요됩니다. L3 캐시는 수 MB에서 수십 MB로 수십 사이클이 걸립니다. 메인 메모리는 GB 단위이며 수백 사이클이 필요합니다. 디스크는 TB 단위이지만 수백만 사이클이 소요됩니다.

**메모리 계층성의 원리**

지역성의 원리에 기반합니다. 시간적 지역성은 최근 접근한 데이터를 다시 접근할 가능성이 높다는 것입니다. 공간적 지역성은 인접한 주소의 데이터를 연속적으로 접근할 가능성이 높다는 것입니다. 캐시는 자주 사용되는 데이터를 빠른 메모리에 저장하여 평균 접근 시간을 크게 단축합니다.

**캐시의 역할**

CPU와 메인 메모리 사이의 속도 차이를 완화합니다. 메모리 접근의 대부분을 캐시에서 처리하여 성능을 향상시킵니다. 캐시 히트율이 90% 이상이면 평균 접근 시간이 메모리 접근 시간의 10분의 1 수준으로 줄어듭니다. 현대 프로세서는 캐시 없이는 제 성능을 발휘할 수 없습니다.

**포괄적 계층과 비포괄적 계층**

포괄적 캐시는 상위 캐시의 모든 데이터가 하위 캐시에도 존재합니다. L1의 데이터가 L2에도 있어서 일관성 유지가 쉽지만 중복으로 공간이 낭비됩니다. 비포괄적 캐시는 각 레벨이 독립적인 데이터를 가져서 전체 캐시 용량이 증가하지만 관리가 복잡합니다. 최신 CPU는 하이브리드 방식을 사용하기도 합니다.

**메모리 벽 문제**

CPU 성능은 매우 빠르게 향상되었지만, 메모리 속도는 상대적으로 느리게 발전했습니다. 이로 인한 성능 격차를 메모리 벽이라고 합니다. 캐시 계층을 늘리고, 프리페칭으로 미리 데이터를 가져오며, 대역폭을 늘려서 완화합니다. 하지만 근본적 해결은 어렵고, 소프트웨어 최적화도 중요합니다.

**프로그래머의 역할**

메모리 계층성을 이해하고 캐시 친화적인 코드를 작성해야 합니다. 데이터 구조를 메모리에 연속적으로 배치하여 공간 지역성을 높입니다. 루프 블로킹으로 작업 세트를 캐시에 맞춥니다. 불필요한 메모리 접근을 줄이고, 데이터 재사용을 극대화합니다. 캐시 라인 크기를 고려하여 자료구조를 설계합니다.

**실무 최적화**

배열 순회는 행 우선 순서로 하여 캐시 히트율을 높입니다. 구조체 배열보다 배열의 구조체가 캐시에 유리한 경우가 많습니다. 루프 융합으로 데이터 재사용을 늘립니다. 프리페칭 힌트를 주거나, 컴파일러 최적화 옵션을 활용합니다. 성능 프로파일러로 캐시 미스를 측정하고 개선합니다.

---

## 76. 캐시 메모리는 어디에 위치해 있나요?

**기본 개념**

캐시 메모리는 CPU와 메인 메모리 사이에 위치하며, 물리적으로는 CPU 칩 내부나 매우 가까운 곳에 있습니다. 계층에 따라 위치와 특성이 다릅니다.

**L1 캐시의 위치**

L1 캐시는 CPU 코어 내부에 위치합니다. 각 코어마다 독립적인 L1 캐시를 가지며, 명령어 캐시와 데이터 캐시로 분리된 하버드 아키텍처를 사용합니다. 가장 빠르게 접근할 수 있지만 용량이 작습니다. 일반적으로 32KB에서 64KB 정도입니다. CPU 다이 면적의 상당 부분을 차지하며, 매우 비싼 SRAM으로 구성됩니다.

**L2 캐시의 위치**

L2 캐시는 CPU 코어에 전용으로 붙어 있거나, 여러 코어가 공유하는 형태입니다. 최신 아키텍처에서는 각 코어에 전용 L2 캐시를 제공하는 추세입니다. L1보다 느리지만 용량이 크며, 일반적으로 256KB에서 512KB 정도입니다. 역시 CPU 칩 내부에 위치하여 빠른 접근이 가능합니다.

**L3 캐시의 위치**

L3 캐시는 모든 코어가 공유하는 대용량 캐시로, CPU 칩 내부에 위치하지만 코어들과는 약간 떨어져 있습니다. 수 MB에서 수십 MB의 큰 용량을 가지며, 코어 간 데이터 공유를 효율적으로 지원합니다. 한 코어가 수정한 데이터를 다른 코어가 빠르게 접근할 수 있습니다. 서버급 CPU는 64MB 이상의 L3 캐시를 탑재하기도 합니다.

**온다이와 오프다이**

대부분의 캐시는 온다이, 즉 CPU 다이에 통합되어 있습니다. 이는 매우 빠른 접근 속도를 제공하지만, 다이 크기와 제조 비용을 증가시킵니다. 일부 구형 시스템에서는 L2나 L3 캐시가 별도 칩으로 존재하는 오프다이 구조였습니다. 현대 CPU는 거의 모두 온다이 통합 방식을 사용합니다.

**멀티코어와 캐시 배치**

멀티코어 CPU에서 L1과 L2는 각 코어 전용이고, L3는 공유되는 구조가 일반적입니다. 이는 코어별 독립적인 작업에는 빠른 L1/L2를 사용하고, 코어 간 통신에는 L3를 활용하는 균형잡힌 설계입니다. 캐시 일관성 프로토콜로 코어 간 데이터 일관성을 유지합니다.

**메모리 컨트롤러와의 관계**

현대 CPU는 메모리 컨트롤러도 통합하여 CPU 칩 내부에 위치시킵니다. 이는 CPU와 메모리 사이의 지연 시간을 줄이고 대역폭을 늘립니다. L3 캐시와 메모리 컨트롤러는 링 버스나 메시 네트워크로 연결되어 효율적으로 통신합니다.

**NUMA와 캐시**

서버 시스템의 NUMA 아키텍처에서는 각 소켓마다 독립적인 메모리와 캐시를 가집니다. 로컬 메모리 접근이 원격 메모리 접근보다 빠르며, 캐시도 로컬 메모리에 최적화됩니다. 프로그램은 NUMA 인식 방식으로 작성되어야 최적 성능을 얻을 수 있습니다.

**실무 의미**

캐시가 CPU 내부에 있다는 것은 매우 빠른 접근을 의미하지만, 공간이 제한적이라는 뜻입니다. 프로그래머는 작업 세트를 캐시 크기 내로 유지하도록 노력해야 합니다. perf나 VTune 같은 도구로 캐시 사용 현황을 분석할 수 있습니다. 멀티코어 프로그래밍에서는 false sharing을 피하기 위해 캐시 라인 경계를 고려해야 합니다.
